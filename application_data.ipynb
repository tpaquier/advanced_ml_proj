{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "! pip install cvxpy\n",
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_path = os.getcwd()\n",
    "os.chdir('/home/onyxia/work/advanced_ml_proj/data')\n",
    "original_data = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "os.chdir(actual_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_names = []\n",
    "for name in list(original_data.columns):\n",
    "    temp_name = name.split()\n",
    "    full_names.append(temp_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.columns = full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.drop(['url','timedelta'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data['shares'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = original_data.shape[0]\n",
    "original_data['pop_cat'] = original_data['shares'].copy()\n",
    "data = original_data.copy()\n",
    "for i in range(n):\n",
    "    if data.loc[i,'shares'] < 946.:\n",
    "        data.loc[i,'pop_cat'] = 0\n",
    "    elif data.loc[i,'shares'] < 1400. :\n",
    "        data.loc[i,'pop_cat'] = 1\n",
    "    elif data.loc[i,'shares'] < 2800. :\n",
    "        data.loc[i,'pop_cat'] = 2\n",
    "    elif data.loc[i,'shares'] >= 2800. :\n",
    "        data.loc[i,'pop_cat'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_ker(x, y, q):\n",
    "    \"\"\"a function to compute the gaussian kernel of two points\n",
    "    -------------------------------\n",
    "    inputs : \n",
    "\n",
    "    x : array-like, vector\n",
    "    first vector for which we want to compute the kernel\n",
    "\n",
    "    y : array-like, vector\n",
    "    second vector for which we want to compute the kernel\n",
    "\n",
    "    q : positive float,\n",
    "    value of the bandwidth of the kernel\n",
    "\n",
    "    returns:\n",
    "    ker : float,\n",
    "    the value of the kernel\n",
    "    -------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    ker = np.exp(-q*np.linalg.norm(x-y)**2)\n",
    "    return ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_mat(X, q):\n",
    "    \"\"\"\n",
    "    a function to compute the gram matrix of a given dataset\n",
    "\n",
    "    ----------------------------------------\n",
    "    inputs : \n",
    "    X : array-like object, must be 2D\n",
    "    the data for which we want to compute the gram matrix\n",
    "\n",
    "    q : positive float, \n",
    "    the bandwidth of the gaussian kernel\n",
    "    -----------------------------------------\n",
    "\n",
    "    returns:\n",
    "    K : the gram matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    norms = np.linalg.norm(X, axis=1)**2\n",
    "    dot = X@X.T\n",
    "    squared_euclidian_distances = norms[:, None] - 2 * dot + norms[None, :]\n",
    "    K = np.exp(-squared_euclidian_distances*q)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seg(x, y, nb=20):\n",
    "    \"\"\"\n",
    "    a function used to compute the segment between two points\n",
    "    -----------------------------------\n",
    "    Parameters : \n",
    "\n",
    "    x : array-like obj,\n",
    "    an input, d>=2\n",
    "\n",
    "    y : array-like obj, \n",
    "    the second input\n",
    "\n",
    "    nb : int, \n",
    "    the number of points we want to have between the two points\n",
    "\n",
    "    Returns : \n",
    "\n",
    "    segment : array-like\n",
    "    an array of shape d (dimension of x), nb\n",
    "    ---------------------------------------\n",
    "    \"\"\"\n",
    "    d = x.shape[0]\n",
    "    segment = np.zeros((nb, d))\n",
    "    points = np.linspace(start=0., stop=1., num=nb, endpoint=True)\n",
    "    \n",
    "    for i in range(nb):\n",
    "        t = points[i]\n",
    "        segment[i, :] = (1-t) * x + t * y\n",
    "        \n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radius(x, sample, beta, bkb, q, ker_self=1.):\n",
    "    \"\"\"\n",
    "    compute the radius for a given instance x\n",
    "    -----------------------------------------------\n",
    "    Parameters : \n",
    "\n",
    "    x : 1-D vector,\n",
    "    the input vector\n",
    "\n",
    "    sample : matrix, \n",
    "    the whole sample, \n",
    "\n",
    "    beta : array-like, \n",
    "    the calculated beta\n",
    "\n",
    "    bkb : float,\n",
    "    the result of beta.T@K@beta\n",
    "\n",
    "    ker_self : float,\n",
    "    the value of the kernel of the selected instance with itself, \n",
    "    set to 1 by default as we use mostly the gaussian kernel\n",
    "\n",
    "    returns :\n",
    "\n",
    "    radius : float,\n",
    "    the distance between the test instance and the center of the \n",
    "    sphere enclosing all the points in the Hilbert space\n",
    "    -----------------------------------------------\n",
    "    \"\"\"\n",
    "    nb_samp = sample.shape[0]\n",
    "    temp_k = np.zeros(nb_samp)\n",
    "\n",
    "    for elem in range(nb_samp):\n",
    "        temp_k[elem] = gaussian_ker(x, sample[elem,:], q=q)\n",
    "        \n",
    "    return np.sqrt(ker_self - 2*np.dot(temp_k, beta) + bkb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['pop_cat'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#we center the data for the pca (as done in the article)\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop('pop_cat', axis=1))\n",
    "\n",
    "\n",
    "n_components = 15  \n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "columns = [f\"PC{i+1}\" for i in range(n_components)]\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=columns)\n",
    "\n",
    "\n",
    "print(\"part of variance explained by every component :\", pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "print(\"total variance explained :\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "d = principal_df.shape[1]\n",
    "principal_df = np.hstack((principal_df,target.to_numpy().reshape(-1,1)))\n",
    "principal_df_whole = principal_df.copy()\n",
    "principal_df = pd.DataFrame(principal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we rename the columns (more of a commidity than a necessity)\n",
    "colnames = []\n",
    "for i in range(15):\n",
    "    colnames.append(f'pc_{i}')\n",
    "colnames.append('target')\n",
    "principal_df.columns = colnames\n",
    "principal_df = principal_df.groupby('target', group_keys=False).apply(lambda x: x.sample(frac=0.01,random_state=87876899))\n",
    "target = principal_df['target'].copy()\n",
    "print(principal_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_df.drop('target',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a quick plot to see to what extent does the pca contributes to the possibility of \n",
    "#separation of the data (here on the first two components) \n",
    "plt.scatter(principal_df.iloc[:, 0], - principal_df.iloc[:, 1], marker='x', color='blue', s=20, linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the SVC procedure\n",
    "N = n\n",
    "q = 2\n",
    "p = 0.1\n",
    "C = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_x = gram_mat(X=principal_df, q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(principal_df)\n",
    "beta = cp.Variable(n)\n",
    "gram_x += gram_x.T\n",
    "gram_x /= 2\n",
    "gram_x = cp.psd_wrap(gram_x)\n",
    "\n",
    "\n",
    "# Formulation de l'objectif\n",
    "objective = cp.Maximize(cp.sum(np.ones(n) @ beta) - cp.quad_form(beta, gram_x))\n",
    "\n",
    "# Contraintes\n",
    "constraints = [\n",
    "    beta >= 0,  # 0 <= beta_j\n",
    "    beta <= C,  # beta_j <= C\n",
    "    cp.sum(beta) == 1  # La somme des éléments de beta doit être égale à 1\n",
    "]\n",
    "\n",
    "# Définir le problème d'optimisation\n",
    "problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# Résoudre le problème\n",
    "problem.solve(solver='CLARABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta = beta.value\n",
    "beta_k_beta = true_beta.T @ gram_x @ true_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_sv = []\n",
    "index_of_bsv = []\n",
    "N = principal_df.shape[0]\n",
    "for i in range(N):\n",
    "    if 1e-10 < true_beta[i] and true_beta[i] < C:\n",
    "        index_of_sv.append(i)\n",
    "    elif true_beta[i] >= C - 1e-3:\n",
    "        index_of_bsv.append(i)\n",
    "\n",
    "print('index of sv', index_of_sv)\n",
    "print('number of sv', len(index_of_sv))\n",
    "print('index of bsv', index_of_bsv)\n",
    "print('number of bsv', len(index_of_bsv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_sv = principal_df.iloc[index_of_sv, :].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_x = gram_mat(X=principal_df, q=q)\n",
    "gram_x += gram_x.T\n",
    "gram_x /= 2\n",
    "r = []\n",
    "beta_k_beta = true_beta.T @ gram_x @ true_beta\n",
    "for point_sv in potential_sv:\n",
    "    temp_K = np.zeros(N)\n",
    "    for elem in range(N):\n",
    "        temp_K[elem] = gaussian_ker(point_sv, principal_df.iloc[elem, :], q=q)\n",
    "    r_xi = np.sqrt(1 - 2 * np.dot(temp_K, true_beta) + beta_k_beta)\n",
    "    r.append(r_xi)\n",
    "rad = np.mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = principal_df.shape[0]\n",
    "adjacency_mat = np.zeros((n, n))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    for j in range(i+1, n):\n",
    "        decision = True\n",
    "        segment = compute_seg(x=principal_df.to_numpy()[i,:], y=principal_df.to_numpy()[j,:])\n",
    "        list_of_val = []\n",
    "        for point in segment:\n",
    "            dist = radius(x=point, beta=true_beta, bkb=beta_k_beta, sample=principal_df.to_numpy(), q=q)\n",
    "            list_of_val.append(dist)\n",
    "        for value in list_of_val:\n",
    "            if value > rad:\n",
    "                decision = False\n",
    "        \n",
    "        if decision == True:\n",
    "            adjacency_mat[i, j] = 1\n",
    "\n",
    "adjacency_mat = adjacency_mat + adjacency_mat.T\n",
    "adjacency_mat /= 2\n",
    "for i in range(n):\n",
    "    adjacency_mat[i,i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_array(adjacency_mat)\n",
    "\n",
    "clusters = list(nx.connected_components(G))\n",
    "print('number of clusters detected', len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_clusters = sorted(clusters, key=len, reverse=True)\n",
    "print(f\"largest cluster: {len(sorted_clusters[0])}\")\n",
    "\n",
    "#a quick preview of how we are doing\n",
    "top_5_clusters = sorted_clusters[:5]\n",
    "for i, cluster in enumerate(top_5_clusters, start=1):\n",
    "    print(f\"size of cluster {i}: {len(cluster)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster labelling\n",
    "misclassified_count = 0\n",
    "for cluster in clusters:\n",
    "\n",
    "    if len(cluster) > 1:\n",
    "\n",
    "        cluster_labels = [target[i] for i in cluster]\n",
    "        majority_label = Counter(cluster_labels).most_common(1)[0][0]\n",
    "        for i in cluster:\n",
    "            if target[i] != majority_label:\n",
    "                misclassified_count += 1\n",
    "\n",
    "print(f\"total number of missclassification: {misclassified_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
