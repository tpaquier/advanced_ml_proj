{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7354.34s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cvxpy in /opt/conda/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (0.6.7.post3)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (0.9.0)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (3.2.7.post2)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (1.15.1)\n",
      "Requirement already satisfied: qdldl in /opt/conda/lib/python3.12/site-packages (from osqp>=0.6.2->cvxpy) (0.1.7.post5)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "! pip install cvxpy\n",
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_ker(x, y, q):\n",
    "    \"\"\"a function to compute the gaussian kernel of two points\n",
    "    -------------------------------\n",
    "    inputs : \n",
    "\n",
    "    x : array-like, vector\n",
    "    first vector for which we want to compute the kernel\n",
    "\n",
    "    y : array-like, vector\n",
    "    second vector for which we want to compute the kernel\n",
    "\n",
    "    q : positive float,\n",
    "    value of the bandwidth of the kernel\n",
    "\n",
    "    returns:\n",
    "    ker : float,\n",
    "    the value of the kernel\n",
    "    -------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    ker = np.exp(-q*np.linalg.norm(x-y)**2)\n",
    "    return ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_mat(X, q):\n",
    "    \"\"\"\n",
    "    a function to compute the gram matrix of a given dataset\n",
    "\n",
    "    ----------------------------------------\n",
    "    inputs : \n",
    "    X : array-like object, must be 2D\n",
    "    the data for which we want to compute the gram matrix\n",
    "\n",
    "    q : positive float, \n",
    "    the bandwidth of the gaussian kernel\n",
    "    -----------------------------------------\n",
    "\n",
    "    returns:\n",
    "    K : the gram matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    norms = np.linalg.norm(X, axis=1)**2\n",
    "    dot = X@X.T\n",
    "    squared_euclidian_distances = norms[:, None] - 2 * dot + norms[None, :]\n",
    "    K = np.exp(-squared_euclidian_distances*q)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seg(x, y, nb=20):\n",
    "    \"\"\"\n",
    "    a function used to compute the segment between two points\n",
    "    -----------------------------------\n",
    "    Parameters : \n",
    "\n",
    "    x : array-like obj,\n",
    "    an input, d>=2\n",
    "\n",
    "    y : array-like obj, \n",
    "    the second input\n",
    "\n",
    "    nb : int, \n",
    "    the number of points we want to have between the two points\n",
    "\n",
    "    Returns : \n",
    "\n",
    "    segment : array-like\n",
    "    an array of shape d (dimension of x), nb\n",
    "    ---------------------------------------\n",
    "    \"\"\"\n",
    "    d = x.shape[0]\n",
    "    segment = np.zeros((nb, d))\n",
    "    points = np.linspace(start=0., stop=1., num=nb, endpoint=True)\n",
    "    \n",
    "    for i in range(nb):\n",
    "        t = points[i]\n",
    "        segment[i, :] = (1-t) * x + t * y\n",
    "        \n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radius(x, sample, beta, bkb, q, ker_self=1.):\n",
    "    \"\"\"\n",
    "    compute the radius for a given instance x\n",
    "    -----------------------------------------------\n",
    "    Parameters : \n",
    "\n",
    "    x : 1-D vector,\n",
    "    the input vector\n",
    "\n",
    "    sample : matrix, \n",
    "    the whole sample, \n",
    "\n",
    "    beta : array-like, \n",
    "    the calculated beta\n",
    "\n",
    "    bkb : float,\n",
    "    the result of beta.T@K@beta\n",
    "\n",
    "    ker_self : float,\n",
    "    the value of the kernel of the selected instance with itself, \n",
    "    set to 1 by default as we use mostly the gaussian kernel\n",
    "\n",
    "    returns :\n",
    "\n",
    "    radius : float,\n",
    "    the distance between the test instance and the center of the \n",
    "    sphere enclosing all the points in the Hilbert space\n",
    "    -----------------------------------------------\n",
    "    \"\"\"\n",
    "    nb_samp = sample.shape[0]\n",
    "    temp_k = np.zeros(nb_samp)\n",
    "\n",
    "    for elem in range(nb_samp):\n",
    "        temp_k[elem] = gaussian_ker(x, sample[elem], q=q)\n",
    "        \n",
    "    return np.sqrt(ker_self - 2*np.dot(temp_k, beta) + bkb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          PC1       PC2       PC3\n",
      "0   -2.264703  0.480027  0.127706\n",
      "1   -2.080961 -0.674134  0.234609\n",
      "2   -2.364229 -0.341908 -0.044201\n",
      "3   -2.299384 -0.597395 -0.091290\n",
      "4   -2.389842  0.646835 -0.015738\n",
      "..        ...       ...       ...\n",
      "145  1.870503  0.386966 -0.256274\n",
      "146  1.564580 -0.896687  0.026371\n",
      "147  1.521170  0.269069 -0.180178\n",
      "148  1.372788  1.011254 -0.933395\n",
      "149  0.960656 -0.024332 -0.528249\n",
      "\n",
      "[150 rows x 3 columns]\n",
      "Variance expliquée par chaque composante : [0.72962445 0.22850762 0.03668922]\n",
      "Variance totale expliquée : 0.9948212908928451\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prétraitement : Normaliser les données (centrage et réduction)\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Initialisation de la PCA\n",
    "n_components = 3  # Choisissez le nombre de composantes principales souhaitées\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Appliquer la PCA\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Créer un DataFrame des composantes principales\n",
    "columns = [f\"PC{i+1}\" for i in range(n_components)]\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=columns)\n",
    "\n",
    "# Afficher les composantes principales\n",
    "print(principal_df)\n",
    "\n",
    "# Variance expliquée par chaque composante\n",
    "print(\"Variance expliquée par chaque composante :\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Somme de la variance expliquée\n",
    "print(\"Variance totale expliquée :\", np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the SVC procedure\n",
    "N = len(principal_df)\n",
    "q = 7\n",
    "p = 0.7\n",
    "C = 1 / (N * p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_x = gram_mat(X=principal_df, q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9798588124462496)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(principal_df)\n",
    "beta = cp.Variable(n)\n",
    "gram_x += gram_x.T\n",
    "gram_x /= 2\n",
    "gram_x = cp.psd_wrap(gram_x)\n",
    "\n",
    "\n",
    "# Formulation de l'objectif\n",
    "objective = cp.Maximize(cp.sum(np.ones(n) @ beta) - cp.quad_form(beta, gram_x))\n",
    "\n",
    "# Contraintes\n",
    "constraints = [\n",
    "    beta >= 0,  # 0 <= beta_j\n",
    "    beta <= C,  # beta_j <= C\n",
    "    cp.sum(beta) == 1  # La somme des éléments de beta doit être égale à 1\n",
    "]\n",
    "\n",
    "# Définir le problème d'optimisation\n",
    "problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# Résoudre le problème\n",
    "problem.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta = beta.value\n",
    "beta_k_beta = true_beta.T @ gram_x @ true_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 9, 11, 16, 20, 21, 26, 28, 31, 38, 45, 46, 47, 49, 54, 55, 58, 61, 71, 73, 74, 75, 78, 80, 88, 90, 92, 94, 101, 103, 112, 116, 126, 127, 128, 133, 134, 137, 138, 139, 140, 142]\n"
     ]
    }
   ],
   "source": [
    "index_of_sv = []\n",
    "\n",
    "for i in range(N):\n",
    "    if 1e-10 < true_beta[i] < C - 1e-3:\n",
    "        index_of_sv.append(i)\n",
    "\n",
    "print(index_of_sv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_sv = principal_df.iloc[index_of_sv, :].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of potential SVs: 43\n"
     ]
    }
   ],
   "source": [
    "print('number of potential SVs:', len(potential_sv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    if true_beta[i] < 1e-10:\n",
    "        true_beta[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_x = gram_mat(X=principal_df, q=q)\n",
    "gram_x += gram_x.T\n",
    "gram_x /= 2\n",
    "r = []\n",
    "beta_k_beta = true_beta.T @ gram_x @ true_beta\n",
    "for i in potential_sv:\n",
    "    temp_K = np.zeros(n)\n",
    "    for elem in range(n):\n",
    "        temp_K[elem] = gaussian_ker(i, principal_df.iloc[elem, :], q=q)\n",
    "    r_xi = np.sqrt(1 - 2*np.dot(temp_K, true_beta) + beta_k_beta)\n",
    "    r.append(r_xi)\n",
    "rad = max(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:35<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "adjacency_mat = np.zeros((n, n))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    for j in range(i+1, n):\n",
    "        decision = True\n",
    "        segment = compute_seg(x=principal_df.to_numpy()[i,:], y=principal_df.to_numpy()[j,:])\n",
    "        list_of_val = []\n",
    "        for point in segment:\n",
    "            dist = radius(x=point, beta=true_beta, bkb=beta_k_beta, sample=principal_df.to_numpy(), q=q)\n",
    "            list_of_val.append(dist)\n",
    "        for value in list_of_val:\n",
    "            if value > rad:\n",
    "                decision = False\n",
    "        \n",
    "        if decision == True:\n",
    "            adjacency_mat[i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_mat = adjacency_mat + adjacency_mat.T\n",
    "adjacency_mat /= 2\n",
    "for i in range(n):\n",
    "    adjacency_mat[i,i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.from_numpy_array(adjacency_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = list(nx.connected_components(G))\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du plus gros cluster: 44\n",
      "Taille du cluster 1: 44\n",
      "Taille du cluster 2: 27\n",
      "Taille du cluster 3: 8\n",
      "Taille du cluster 4: 1\n",
      "Taille du cluster 5: 1\n"
     ]
    }
   ],
   "source": [
    "# Trier les clusters par taille décroissante\n",
    "sorted_clusters = sorted(clusters, key=len, reverse=True)\n",
    "\n",
    "# Afficher la taille du plus gros cluster\n",
    "print(f\"Taille du plus gros cluster: {len(sorted_clusters[0])}\")\n",
    "\n",
    "# Si vous voulez afficher les tailles des 5 plus gros clusters, par exemple :\n",
    "top_5_clusters = sorted_clusters[:5]\n",
    "for i, cluster in enumerate(top_5_clusters, start=1):\n",
    "    print(f\"Taille du cluster {i}: {len(cluster)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de misclassifications : 19\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "misclassified_count = 0\n",
    "\n",
    "# Parcourir chaque cluster\n",
    "for cluster in clusters:\n",
    "\n",
    "    if len(cluster) > 1:\n",
    "        # Extraire les labels des points dans le cluster\n",
    "        cluster_labels = [target[i] for i in cluster]\n",
    "        \n",
    "        # Trouver le label majoritaire dans le cluster\n",
    "        majority_label = Counter(cluster_labels).most_common(1)[0][0]\n",
    "        \n",
    "        # Compter les misclassifications dans ce cluster\n",
    "        for i in cluster:\n",
    "            if target[i] != majority_label:\n",
    "                misclassified_count += 1\n",
    "\n",
    "print(f\"Nombre total de misclassifications : {misclassified_count}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
