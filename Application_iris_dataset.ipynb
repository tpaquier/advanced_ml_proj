{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "! pip install cvxpy\n",
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_ker(x, y, q):\n",
    "    \"\"\"a function to compute the gaussian kernel of two points\n",
    "    -------------------------------\n",
    "    inputs : \n",
    "\n",
    "    x : array-like, vector\n",
    "    first vector for which we want to compute the kernel\n",
    "\n",
    "    y : array-like, vector\n",
    "    second vector for which we want to compute the kernel\n",
    "\n",
    "    q : positive float,\n",
    "    value of the bandwidth of the kernel\n",
    "\n",
    "    returns:\n",
    "    ker : float,\n",
    "    the value of the kernel\n",
    "    -------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    ker = np.exp(-q*np.linalg.norm(x-y)**2)\n",
    "    return ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_mat(X, q):\n",
    "    \"\"\"\n",
    "    a function to compute the gram matrix of a given dataset\n",
    "\n",
    "    ----------------------------------------\n",
    "    inputs : \n",
    "    X : array-like object, must be 2D\n",
    "    the data for which we want to compute the gram matrix\n",
    "\n",
    "    q : positive float, \n",
    "    the bandwidth of the gaussian kernel\n",
    "    -----------------------------------------\n",
    "\n",
    "    returns:\n",
    "    K : the gram matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    norms = np.linalg.norm(X, axis=1)**2\n",
    "    dot = X@X.T\n",
    "    squared_euclidian_distances = norms[:, None] - 2 * dot + norms[None, :]\n",
    "    K = np.exp(-squared_euclidian_distances*q)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seg(x, y, nb=20):\n",
    "    \"\"\"\n",
    "    a function used to compute the segment between two points\n",
    "    -----------------------------------\n",
    "    Parameters : \n",
    "\n",
    "    x : array-like obj,\n",
    "    an input, d>=2\n",
    "\n",
    "    y : array-like obj, \n",
    "    the second input\n",
    "\n",
    "    nb : int, \n",
    "    the number of points we want to have between the two points\n",
    "\n",
    "    Returns : \n",
    "\n",
    "    segment : array-like\n",
    "    an array of shape d (dimension of x), nb\n",
    "    ---------------------------------------\n",
    "    \"\"\"\n",
    "    d = x.shape[0]\n",
    "    segment = np.zeros((nb, d))\n",
    "    points = np.linspace(start=0., stop=1., num=nb, endpoint=True)\n",
    "    points /= nb\n",
    "    \n",
    "    for i in range(nb):\n",
    "        t = points[i]\n",
    "        segment[i, :] = (1-t) * x + t * y\n",
    "        \n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radius(x, sample, beta, bkb, q, ker_self=1.):\n",
    "    \"\"\"\n",
    "    compute the radius for a given instance x\n",
    "    -----------------------------------------------\n",
    "    Parameters : \n",
    "\n",
    "    x : 1-D vector,\n",
    "    the input vector\n",
    "\n",
    "    sample : matrix, \n",
    "    the whole sample, \n",
    "\n",
    "    beta : array-like, \n",
    "    the calculated beta\n",
    "\n",
    "    bkb : float,\n",
    "    the result of beta.T@K@beta\n",
    "\n",
    "    ker_self : float,\n",
    "    the value of the kernel of the selected instance with itself, \n",
    "    set to 1 by default as we use mostly the gaussian kernel\n",
    "\n",
    "    returns :\n",
    "\n",
    "    radius : float,\n",
    "    the distance between the test instance and the center of the \n",
    "    sphere enclosing all the points in the Hilbert space\n",
    "    -----------------------------------------------\n",
    "    \"\"\"\n",
    "    nb_samp = sample.shape[0]\n",
    "    temp_k = np.zeros(nb_samp)\n",
    "\n",
    "    for elem in range(nb_samp):\n",
    "        temp_k[elem] = gaussian_ker(x, sample[elem], q=q)\n",
    "        \n",
    "    return np.sqrt(ker_self - 2*np.dot(temp_k, beta) + bkb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance expliquée par chaque composante : [0.72962445 0.22850762]\n",
      "Variance totale expliquée : 0.9581320720000164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prétraitement : Normaliser les données (centrage et réduction)\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Initialisation de la PCA\n",
    "n_components = 2  # Choisissez le nombre de composantes principales souhaitées\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Appliquer la PCA\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Créer un DataFrame des composantes principales\n",
    "columns = [f\"PC{i+1}\" for i in range(n_components)]\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=columns)\n",
    "\n",
    "# Variance expliquée par chaque composante\n",
    "print(\"Variance expliquée par chaque composante :\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Somme de la variance expliquée\n",
    "print(\"Variance totale expliquée :\", np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(principal_df.iloc[:50, 0], - principal_df.iloc[:50, 1], marker='x', color='blue', s=20, linewidths=0.5)\n",
    "plt.scatter(principal_df.iloc[50:100, 0], - principal_df.iloc[50:100, 1], marker='d', color='black', facecolors=\"none\", s=20, linewidths=0.5)\n",
    "plt.scatter(principal_df.iloc[100:, 0], - principal_df.iloc[100:, 1], marker='s', color='red', facecolors=\"none\", s=20, linewidths=0.5)\n",
    "# plt.ylim(-1.5, 1.5)\n",
    "# plt.xlim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the SVC procedure\n",
    "N = 150\n",
    "q = 7\n",
    "p = 0.7\n",
    "C = 1/(N*p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_x = gram_mat(X=principal_df, q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(principal_df)\n",
    "beta = cp.Variable(n)\n",
    "gram_x += gram_x.T\n",
    "gram_x /= 2\n",
    "gram_x = cp.psd_wrap(gram_x)\n",
    "\n",
    "\n",
    "# Formulation de l'objectif\n",
    "objective = cp.Maximize(cp.sum(np.ones(n) @ beta) - cp.quad_form(beta, gram_x))\n",
    "\n",
    "# Contraintes\n",
    "constraints = [\n",
    "    beta >= 0,  # 0 <= beta_j\n",
    "    beta <= C,  # beta_j <= C\n",
    "    cp.sum(beta) == 1  # La somme des éléments de beta doit être égale à 1\n",
    "]\n",
    "\n",
    "# Définir le problème d'optimisation\n",
    "problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# Résoudre le problème\n",
    "problem.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta = beta.value\n",
    "beta_k_beta = true_beta.T @ gram_x @ true_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_sv = []\n",
    "index_of_bsv = []\n",
    "\n",
    "for i in range(N):\n",
    "    if 1e-10 < true_beta[i] and true_beta[i] < C:\n",
    "        index_of_sv.append(i)\n",
    "    elif true_beta[i] >= C - 1e-3:\n",
    "        index_of_bsv.append(i)\n",
    "\n",
    "print('index of sv', index_of_sv)\n",
    "print('number of sv', len(index_of_sv))\n",
    "print('index of bsv', index_of_bsv)\n",
    "print('number of bsv', len(index_of_bsv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_sv = principal_df.iloc[index_of_sv, :].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_x = gram_mat(X=principal_df, q=q)\n",
    "gram_x += gram_x.T\n",
    "gram_x /= 2\n",
    "r = []\n",
    "beta_k_beta = true_beta.T @ gram_x @ true_beta\n",
    "for point_sv in potential_sv:\n",
    "    temp_K = np.zeros(n)\n",
    "    for elem in range(n):\n",
    "        temp_K[elem] = gaussian_ker(point_sv, principal_df.iloc[elem, :], q=q)\n",
    "    r_xi = np.sqrt(1 - 2 * np.dot(temp_K, true_beta) + beta_k_beta)\n",
    "    r.append(r_xi)\n",
    "rad = np.mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_mat = np.zeros((n, n))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    for j in range(i+1, n):\n",
    "        decision = True\n",
    "        segment = compute_seg(x=principal_df.to_numpy()[i,:], y=principal_df.to_numpy()[j,:])\n",
    "        list_of_val = []\n",
    "        for point in segment:\n",
    "            dist = radius(x=point, beta=true_beta, bkb=beta_k_beta, sample=principal_df.to_numpy(), q=q)\n",
    "            list_of_val.append(dist)\n",
    "        for value in list_of_val:\n",
    "            if value > rad:\n",
    "                decision = False\n",
    "        \n",
    "        if decision == True:\n",
    "            adjacency_mat[i, j] = 1\n",
    "\n",
    "adjacency_mat = adjacency_mat + adjacency_mat.T\n",
    "adjacency_mat /= 2\n",
    "for i in range(n):\n",
    "    adjacency_mat[i,i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.from_numpy_array(adjacency_mat)\n",
    "\n",
    "clusters = list(nx.connected_components(G))\n",
    "print('number of clusters detected', len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier les clusters par taille décroissante\n",
    "sorted_clusters = sorted(clusters, key=len, reverse=True)\n",
    "\n",
    "# Afficher la taille du plus gros cluster\n",
    "print(f\"Taille du plus gros cluster: {len(sorted_clusters[0])}\")\n",
    "\n",
    "# Si vous voulez afficher les tailles des 5 plus gros clusters, par exemple :\n",
    "top_5_clusters = sorted_clusters[:5]\n",
    "for i, cluster in enumerate(top_5_clusters, start=1):\n",
    "    print(f\"Taille du cluster {i}: {len(cluster)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "misclassified_count = 0\n",
    "\n",
    "# Parcourir chaque cluster\n",
    "for cluster in clusters:\n",
    "\n",
    "    if len(cluster) > 1:\n",
    "        # Extraire les labels des points dans le cluster\n",
    "        cluster_labels = [target[i] for i in cluster]\n",
    "        \n",
    "        # Trouver le label majoritaire dans le cluster\n",
    "        majority_label = Counter(cluster_labels).most_common(1)[0][0]\n",
    "        \n",
    "        # Compter les misclassifications dans ce cluster\n",
    "        for i in cluster:\n",
    "            if target[i] != majority_label:\n",
    "                misclassified_count += 1\n",
    "\n",
    "print(f\"Nombre total de misclassifications : {misclassified_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
